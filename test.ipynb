{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROYECTO: SISTEMA PARA EL SEGUIMIENTO DE ECOSISTEMAS VENEZOLANOS\n",
    "# AUTOR: Javier Martinez\n",
    "\n",
    "# Proceso de experimentos modelos NARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Directorio actual:  /media/javier/Compartida/doctorado/ssev-analytics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print('> Directorio actual: ', os.getcwd())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargando mongo db\n"
     ]
    }
   ],
   "source": [
    "from utils.MONGO import CONEXION\n",
    "from utils.UTILS import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Creando la conexion con MongoDB\n",
    "db = CONEXION.conexion()\n",
    "db.list_collection_names()\n",
    "\n",
    "print('cargando mongo db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parque\n",
    "park = 'cerro_saroche'\n",
    "\n",
    "id_point = 1\n",
    "y_output = 'precip_t'\n",
    "exogena = 'oni'\n",
    "\n",
    "prediction_order = 12 # rango de prediccion\n",
    "auto_order = 10*12 # componente autoregresiva\n",
    "exog_order = 7 # componente exogena qm\n",
    "exog_delay = 4# componente exogena dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = ['tanh','tanh']\n",
    "kernel_initializer = 'lecun_normal'\n",
    "bias_initializer = 'zeros'\n",
    "patience = 15\n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando consulta\n",
    "meteorological = db.meteorological.find({\"park\":park, 'id_point':id_point})\n",
    "\n",
    "# Generando pandas dataframe\n",
    "data_pandas = pd.DataFrame([file for file in meteorological])\n",
    "data_pandas['periodo'] = data_pandas.time.apply(lambda x: datetime.fromordinal(x))\n",
    "data_pandas['mes_year'] =  data_pandas['periodo'].dt.strftime('%B-%Y')\n",
    "data_pandas.index = pd.to_datetime(data_pandas.periodo)\n",
    "\n",
    "DIR = f'./{park}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estudio Precipitacion\n",
    "pd_precipitacion = data_pandas[['id_point', 'latitud', 'longitud',\n",
    "                                'precipitacion_mm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando data SST\n",
    "data_sst = db.estimateSSTNino34.find()\n",
    "\n",
    "# Generando pandas dataframe\n",
    "pd_sst = pd.DataFrame([file for file in data_sst])[['oni','time']]\n",
    "pd_sst['periodo'] = pd_sst.time.apply(lambda x: datetime.fromordinal(x))\n",
    "pd_sst.index = pd.to_datetime(pd_sst.periodo)\n",
    "\n",
    "oni_max = pd_sst.oni.max()\n",
    "oni_min = pd_sst.oni.min()\n",
    "\n",
    "pd_sst['oni'] = pd_sst['oni'].apply(lambda x: (x-oni_min)/(oni_max-oni_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "pd_model = pd.merge(pd_precipitacion.reset_index(drop=False),pd_sst[['oni']].reset_index(drop=False),\n",
    "                    on=['periodo'],\n",
    "                    how='left'\n",
    "                    )\n",
    "\n",
    "# Pronostico\n",
    "pd_sst_pron = pd_sst[['periodo','oni']][pd_sst.periodo > pd_model.periodo.max()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion\n",
    "transformacion = LogMinimax.create( pd_precipitacion.precipitacion_mm.to_numpy() )\n",
    "pd_model['precip_t'] = transformacion.transformacion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo segun ID point\n",
    "pd_model_id = pd_model[pd_model.id_point==id_point]\n",
    "pd_model_id.index = pd.to_datetime(pd_model_id.periodo)\n",
    "pd_model_id = pd_model_id[[y_output,exogena]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo estructura de datos\n",
    "def split_data(pd_model_id,exog_order,auto_order,exog_delay,prediction_order):\n",
    "    \"\"\"\n",
    "    Funcion para dale estructura a los datos\n",
    "    \"\"\"\n",
    "\n",
    "    min_index = max([exog_order,auto_order])\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for t in pd_model_id[min_index:].index:\n",
    "\n",
    "        #t = pd_model_id[min_index:].index.min()\n",
    "\n",
    "        to_split = pd_model_id[[y_output,exogena]]\n",
    "        to_split = to_split[(t-pd.DateOffset(months=auto_order)):t].copy()\n",
    "\n",
    "        # Exogena\n",
    "        x_exo = to_split[exogena][(t-pd.DateOffset(months=exog_delay+exog_order)):(t-pd.DateOffset(months=exog_delay+1))]\\\n",
    "                            .to_numpy()\\\n",
    "                            .astype(float)\\\n",
    "                            .reshape(-1)\n",
    "\n",
    "        # Auto\n",
    "        x_auto = to_split[y_output][:-1]\\\n",
    "                    .to_numpy()\\\n",
    "                    .astype(float)\\\n",
    "                    .reshape(-1)\n",
    "\n",
    "        x_data.append(np.concatenate([x_exo, x_auto],axis=None))\n",
    "        y_data.append(to_split[y_output][-1])\n",
    "\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = split_data(pd_model_id,exog_order,auto_order,exog_delay,prediction_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y validaci√≥n\n",
    "x_train = x_data[:-prediction_order]\n",
    "x_vasl = x_data[-prediction_order:]\n",
    "\n",
    "y_train = y_data[:-prediction_order]\n",
    "y_vasl = y_data[-prediction_order:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo NARX\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Metricas\n",
    "mae = keras.metrics.MeanAbsoluteError()\n",
    "rmse = keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "confi = {'Input':{'batch_size':None,\n",
    "                'name':'input',\n",
    "                'dtype':None,\n",
    "                'sparse':None,\n",
    "                'tensor':None,\n",
    "                'ragged':None,\n",
    "                'type_spec':None},\n",
    "        'Dense':{'use_bias':True,\n",
    "                'kernel_regularizer':None,\n",
    "                'bias_regularizer':None,\n",
    "                'activity_regularizer':None,\n",
    "                'kernel_constraint':None,\n",
    "                'bias_constraint':None\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuronas\n",
    "n_neurons = [int(2*x_train.shape[-1]/3),int(x_train.shape[-1])/3,]\n",
    "\n",
    "# Modelo\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Entradas\n",
    "model.add(keras.layers.Input(shape=(x_train.shape[-1],),\n",
    "                                    batch_size = confi.get('Input').get('batch_size'),\n",
    "                                    name = confi.get('Input').get('name'),\n",
    "                                    dtype = confi.get('Input').get('dtype'),\n",
    "                                    sparse = confi.get('Input').get('sparse'),\n",
    "                                    tensor = confi.get('Input').get('tensor'),\n",
    "                                    ragged = confi.get('Input').get('ragged'),\n",
    "                                    type_spec = confi.get('Input').get('type_spec')\n",
    "                                    ))\n",
    "\n",
    "# Hidden Leyers\n",
    "model.add(keras.layers.Dense(   units=n_neurons[0],\n",
    "                                activation=activation[0],\n",
    "                                use_bias = confi.get('Dense').get('use_bias'),\n",
    "                                kernel_initializer=kernel_initializer,\n",
    "                                bias_initializer=bias_initializer,\n",
    "                                kernel_regularizer = confi.get('Dense').get('kernel_regularizer'),\n",
    "                                bias_regularizer = confi.get('Dense').get('bias_regularizer'),\n",
    "                                activity_regularizer = confi.get('Dense').get('activity_regularizer'),\n",
    "                                kernel_constraint = confi.get('Dense').get('kernel_constraint'),\n",
    "                                bias_constraint = confi.get('Dense').get('bias_constraint')\n",
    "                                ))\n",
    "\n",
    "# Out\n",
    "model.add(keras.layers.Dense(   units=1,\n",
    "                                activation='linear',\n",
    "                                kernel_initializer=kernel_initializer,\n",
    "                                bias_initializer=bias_initializer,\n",
    "                                ))\n",
    "                                \n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[mae,rmse]) \n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "                                            monitor=\"loss\",\n",
    "                                            min_delta=0,\n",
    "                                            patience=patience,\n",
    "                                            verbose=0,\n",
    "                                            mode=\"min\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=False,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio experimento\n",
    "experimento = f'experiments/narx/narx_prec_{id_point}_{len(n_neurons)}_{activation[0]}_{prediction_order}_{auto_order}_{exog_order}_{exog_delay}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(f'{DIR}/{experimento}')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epocas:240\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1,\n",
    "                    verbose=0,\n",
    "                    workers=2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "print(f'Total epocas:{len(history.epoch)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardando experimento\n",
    "model.save(f'{DIR}/{experimento}/narx_precipitacion_model.h5')\n",
    "# Save Pesos\n",
    "model.save_weights(f'{DIR}/{experimento}/narx_precipitacionweights.h5')\n",
    "\n",
    "# Save History\n",
    "import pickle\n",
    "with open(f'{DIR}/{experimento}/narx_precipitacion_history.pkl', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion entrenamiento\n",
    "min_index = max([exog_order,auto_order])\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(x_train, verbose=0).reshape(-1)\n",
    "testPredict = model.predict(x_vasl, verbose=0).reshape(-1)\n",
    "\n",
    "# Data de test\n",
    "trainind_pd = pd.DataFrame(trainPredict,\n",
    "                            index = pd_model_id[min_index:(min_index+trainPredict.shape[0])].index,\n",
    "                            columns=['prediction']\n",
    "                            )\n",
    "                            \n",
    "trainind_pd[y_output] = y_train.reshape(-1)\n",
    "trainind_pd['type'] = 'training'\n",
    "\n",
    "trainind_pd['precipitacion_mm'] = trainind_pd[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "trainind_pd['prediction_precipitacion_mm'] = trainind_pd['prediction'].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "\n",
    "trainind_pd = pd.merge(trainind_pd,pd_model_id[['oni']].reset_index(drop=False),\n",
    "                        on=['periodo'],\n",
    "                        how='left')\n",
    "\n",
    "trainind_pd.index = pd.to_datetime(trainind_pd.periodo)\n",
    "\n",
    "# Validacion\n",
    "trainig_metrics = metrics(observado=trainind_pd.precipitacion_mm,\n",
    "                          prediccion=trainind_pd.prediction_precipitacion_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data de Validacion\n",
    "validation_pd = pd.DataFrame(testPredict,\n",
    "                            index = pd_model_id[(min_index+trainPredict.shape[0]):(min_index+trainPredict.shape[0]+prediction_order)].index,\n",
    "                            columns=['prediction']\n",
    "                            )\n",
    "\n",
    "validation_pd[y_output] = y_vasl.reshape(-1)\n",
    "validation_pd['type'] = 'validation'\n",
    "\n",
    "validation_pd['precipitacion_mm'] = validation_pd[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "validation_pd['prediction_precipitacion_mm'] = validation_pd['prediction'].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "validation_pd = pd.merge(validation_pd,pd_model_id[['oni']].reset_index(drop=False),\n",
    "                        on=['periodo'],\n",
    "                        how='left')\n",
    "\n",
    "# Validacion\n",
    "validation_metrics = metrics(observado=validation_pd.precipitacion_mm,\n",
    "                          prediccion=validation_pd.prediction_precipitacion_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order):\n",
    "  \"\"\"\n",
    "  Funcion para predecir a un paso\n",
    "  \"\"\"\n",
    "  data_predict = data_predict.copy()\n",
    "  data_predict['type'] = 'data_in'\n",
    "\n",
    "  for t in data_exogena.index:\n",
    "    \n",
    "      x_data_test, y_data_test = split_data(data_predict,\n",
    "                                          exog_order,\n",
    "                                          auto_order,\n",
    "                                          exog_delay,\n",
    "                                          prediction_order)\n",
    "\n",
    "\n",
    "      predit = model.predict(x_data_test[-1].reshape(1, x_data_test.shape[1]), verbose=0).reshape(-1)\n",
    "      exo = data_exogena[data_exogena.index==t][exogena][0]\n",
    "\n",
    "      data_test = pd.DataFrame({'periodo':t, y_output:predit, exogena:exo,'type':'data_out'},index=[0])\n",
    "      data_test.index = pd.to_datetime(data_test.periodo)\n",
    "\n",
    "      data_predict = pd.concat([data_predict,\n",
    "                                data_test[list(data_predict)]\n",
    "                              ])\n",
    "\n",
    "  return data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predict = pd_model_id[pd_model_id.index<=trainind_pd.periodo.max()][[y_output,exogena]]\n",
    "data_exogena = pd_model_id[pd_model_id.index>trainind_pd.periodo.max()][[exogena]]\n",
    "\n",
    "pd_test = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order)\n",
    "\n",
    "pd_test = pd_test[pd_test.type=='data_out']\n",
    "pd_test['type'] = 'test'\n",
    "\n",
    "pd_test['prediction'] = pd_test[y_output]\n",
    "pd_test['precipitacion_mm'] = pd_model_id[pd_model_id.index>trainind_pd.periodo.max()][y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "pd_test['prediction_precipitacion_mm'] = pd_test[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "pd_test['precip_t'] = pd_model_id[pd_model_id.index>trainind_pd.periodo.max()][y_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion entrenamiento\n",
    "test_metrics = metrics(observado=pd_test.precipitacion_mm,\n",
    "                        prediccion=pd_test.prediction_precipitacion_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del modelo\n",
    "dict_metrics = {'epocas':[len(history.epoch)],\n",
    "                'prediction_order':[prediction_order],\n",
    "                'auto_order':[auto_order],\n",
    "                'training_mse':[history.history[\"loss\"][-1]],\n",
    "                'training_rmse':[history.history[\"root_mean_squared_error\"][-1]],\n",
    "                'training_mae':[history.history[\"mean_absolute_error\"][-1]],\n",
    "                'trainig_mape':[trainig_metrics['mape']],\n",
    "                'trainig_r':[trainig_metrics['r2']],\n",
    "                'validation_mse':[validation_metrics[\"mse\"]],\n",
    "                'validation_rmse':[validation_metrics[\"rmse\"]],\n",
    "                'validation_mae':[validation_metrics[\"mae\"]],\n",
    "                'validation_mape':[validation_metrics['mape']],\n",
    "                'validation_r':[validation_metrics['r2']],\n",
    "                'test_mse':[test_metrics[\"mse\"]],\n",
    "                'test_rmse':[test_metrics[\"rmse\"]],\n",
    "                'test_mae':[test_metrics[\"mae\"]],\n",
    "                'test_mape':[test_metrics['mape']],\n",
    "                'test_r':[test_metrics['r2']]\n",
    "                }\n",
    "\n",
    "experimento_pd = pd.DataFrame.from_dict(dict_metrics)\n",
    "experimento_pd.to_csv(f'{DIR}/{experimento}/summary.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronostico\n",
    "data_predict = pd_model_id[[y_output,exogena]]\n",
    "data_exogena = pd_sst_pron[pd_sst_pron.index>data_predict.index.max()][[exogena]]\n",
    "\n",
    "pd_prediction = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order)\n",
    "\n",
    "pd_prediction = pd_prediction[pd_prediction.type=='data_out']\n",
    "pd_prediction['type'] = 'prediction'\n",
    "\n",
    "\n",
    "\n",
    "pd_prediction['precipitacion_mm'] = np.nan\n",
    "pd_prediction['prediction_precipitacion_mm'] = pd_prediction[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "pd_prediction['prediction'] = pd_prediction[y_output]\n",
    "pd_prediction['precip_t'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniendo informacion\n",
    "pd_summary = pd.concat([trainind_pd, \n",
    "                        pd_test.reset_index(drop=False), \n",
    "                        pd_prediction.reset_index(drop=False)])\n",
    "pd_summary.index = pd.to_datetime(pd_summary.periodo)\n",
    "\n",
    "pd_summary.to_pickle(f'{DIR}/{experimento}/predicciones.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ssev_analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7962acab993cb47be2d59fc45292cc4ac077641aa9e9bf66ffc44da07d377859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
