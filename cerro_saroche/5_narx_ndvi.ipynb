{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variación espacio-temporal precipitación total\n",
    "\n",
    "**PROYECTO:** SISTEMA PARA EL SEGUIMIENTO DE ECOSISTEMAS VENEZOLANOS \\\n",
    "**AUTOR:** Javier Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Directorio actual:  /media/javier/Compartida/doctorado/ssev-analytics/cerro_saroche\n",
      "> Directorio actual:  /media/javier/Compartida/doctorado/ssev-analytics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print('> Directorio actual: ', os.getcwd())  \n",
    "os.chdir('../')\n",
    "print('> Directorio actual: ', os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils.MONGO import CONEXION\n",
    "from utils.UTILS import *\n",
    "from datetime import datetime\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "park = 'cerro_saroche'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park</th>\n",
       "      <th>periodo</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>id_point</th>\n",
       "      <th>latitud</th>\n",
       "      <th>longitud</th>\n",
       "      <th>type</th>\n",
       "      <th>prediction_ann</th>\n",
       "      <th>ndvi_media</th>\n",
       "      <th>precipitation_ann_t</th>\n",
       "      <th>ndvi_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cerro_saroche</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>training</td>\n",
       "      <td>0.269259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074430</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cerro_saroche</td>\n",
       "      <td>1995-02-01</td>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>training</td>\n",
       "      <td>0.396872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cerro_saroche</td>\n",
       "      <td>1995-03-01</td>\n",
       "      <td>1995</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>training</td>\n",
       "      <td>0.550190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.292917</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cerro_saroche</td>\n",
       "      <td>1995-04-01</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>training</td>\n",
       "      <td>0.706306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.414332</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cerro_saroche</td>\n",
       "      <td>1995-05-01</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>training</td>\n",
       "      <td>0.844658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521932</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            park    periodo  year  month  id_point  latitud  longitud  \\\n",
       "0  cerro_saroche 1995-01-01  1995      1         1    10.31    -69.83   \n",
       "1  cerro_saroche 1995-02-01  1995      2         1    10.31    -69.83   \n",
       "2  cerro_saroche 1995-03-01  1995      3         1    10.31    -69.83   \n",
       "3  cerro_saroche 1995-04-01  1995      4         1    10.31    -69.83   \n",
       "4  cerro_saroche 1995-05-01  1995      5         1    10.31    -69.83   \n",
       "\n",
       "       type  prediction_ann  ndvi_media  precipitation_ann_t  ndvi_t  \n",
       "0  training        0.269259         NaN             0.074430     NaN  \n",
       "1  training        0.396872         NaN             0.173678     NaN  \n",
       "2  training        0.550190         NaN             0.292917     NaN  \n",
       "3  training        0.706306         NaN             0.414332     NaN  \n",
       "4  training        0.844658         NaN             0.521932     NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_precipitacion = pd.read_pickle(f'./{park}/data/ann_precipitacion.pkl')[['park',\n",
    "                                                                            'periodo',\n",
    "                                                                            'year',\n",
    "                                                                            'month',\n",
    "                                                                            'id_point',\n",
    "                                                                            'latitud',\n",
    "                                                                            'longitud',\n",
    "                                                                            'type',\n",
    "                                                                            'prediction_ann',\n",
    "                                                                            'ndvi_media']]\n",
    "\n",
    "\n",
    "# Transformacion\n",
    "ndvi_transformacion = MinMaxScaler() #LogMinimax.create( pd_sst.oni.to_numpy() )\n",
    "ndvi_transformacion.fit(pd_precipitacion[['prediction_ann','ndvi_media']])\n",
    "\n",
    "pd_precipitacion[['precipitation_ann_t','ndvi_t']] = ndvi_transformacion.transform( pd_precipitacion[['prediction_ann','ndvi_media']] )\n",
    "pd_precipitacion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_point = 1\n",
    "\n",
    "exog_order = 12\n",
    "auto_order = 12\n",
    "exog_delay = 0\n",
    "prediction_order = 12\n",
    "\n",
    "y_output = 'ndvi_t'\n",
    "exogena = 'precipitation_ann_t'\n",
    "\n",
    "f_activation = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndvi_t</th>\n",
       "      <th>precipitation_ann_t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>periodo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>0.813947</td>\n",
       "      <td>0.050944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-01</th>\n",
       "      <td>0.631483</td>\n",
       "      <td>0.140930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01</th>\n",
       "      <td>0.447027</td>\n",
       "      <td>0.253160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-01</th>\n",
       "      <td>0.756484</td>\n",
       "      <td>0.371460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-01</th>\n",
       "      <td>0.929545</td>\n",
       "      <td>0.479116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ndvi_t  precipitation_ann_t\n",
       "periodo                                  \n",
       "2012-01-01  0.813947             0.050944\n",
       "2012-02-01  0.631483             0.140930\n",
       "2012-03-01  0.447027             0.253160\n",
       "2012-04-01  0.756484             0.371460\n",
       "2012-05-01  0.929545             0.479116"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_model_id = pd_precipitacion[pd_precipitacion.id_point==id_point]\n",
    "pd_model_id.index = pd.to_datetime(pd_model_id.periodo)\n",
    "pd_model_id = pd_model_id[[y_output,exogena]].dropna().sort_index()\n",
    "pd_model_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorio experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = f'./{park}/'\n",
    "experimento = f'experiments/narx/ndvi/{id_point}'\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'{DIR}{experimento}')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustando modelo NARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = split_data(pd_model_id,exog_order,auto_order,exog_delay,prediction_order,exogena,y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_data[:-prediction_order]\n",
    "x_vasl = x_data[-prediction_order:]\n",
    "\n",
    "y_train = y_data[:-prediction_order]\n",
    "y_vasl = y_data[-prediction_order:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo NARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrícas\n",
    "mae = keras.metrics.MeanAbsoluteError()\n",
    "rmse = keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "confi = {'Input':{'batch_size':None,\n",
    "                'name':'input',\n",
    "                'dtype':None,\n",
    "                'sparse':None,\n",
    "                'tensor':None,\n",
    "                'ragged':None,\n",
    "                'type_spec':None},\n",
    "        'Dense':{'use_bias':True,\n",
    "                'kernel_regularizer':None,\n",
    "                'bias_regularizer':None,\n",
    "                'activity_regularizer':None,\n",
    "                'kernel_constraint':None,\n",
    "                'bias_constraint':None\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = int(2*x_train.shape[-1]/3)\n",
    "n_neurons = [int(3*total/6), int(2*total/6), int(1*total/6), 1]\n",
    "\n",
    "activation = len(n_neurons)*[f_activation]\n",
    "kernel_initializer = 'lecun_normal'\n",
    "bias_initializer = 'zeros'\n",
    "\n",
    "epochs = 20\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Entradas\n",
    "model.add(keras.layers.Input(shape=(x_train.shape[-1],),\n",
    "                                    batch_size = confi.get('Input').get('batch_size'),\n",
    "                                    name = confi.get('Input').get('name'),\n",
    "                                    dtype = confi.get('Input').get('dtype'),\n",
    "                                    sparse = confi.get('Input').get('sparse'),\n",
    "                                    tensor = confi.get('Input').get('tensor'),\n",
    "                                    ragged = confi.get('Input').get('ragged'),\n",
    "                                    type_spec = confi.get('Input').get('type_spec')\n",
    "                                    ))\n",
    "\n",
    "model.add(keras.layers.Dense(   units=n_neurons[0],\n",
    "                                activation=activation[0],\n",
    "                                use_bias = confi.get('Dense').get('use_bias'),\n",
    "                                kernel_initializer=kernel_initializer,\n",
    "                                bias_initializer=bias_initializer,\n",
    "                                kernel_regularizer = confi.get('Dense').get('kernel_regularizer'),\n",
    "                                bias_regularizer = confi.get('Dense').get('bias_regularizer'),\n",
    "                                activity_regularizer = confi.get('Dense').get('activity_regularizer'),\n",
    "                                kernel_constraint = confi.get('Dense').get('kernel_constraint'),\n",
    "                                bias_constraint = confi.get('Dense').get('bias_constraint')\n",
    "                                ))\n",
    "                                \n",
    "model.add(keras.layers.Dropout(0.1))\n",
    "\n",
    "# Hidden Leyers\n",
    "if len(n_neurons)>1:\n",
    "    for index in list( range(1, len(n_neurons)) ):\n",
    "\n",
    "        model.add(keras.layers.Dense(   units=n_neurons[index],\n",
    "                                        activation=activation[index],\n",
    "                                        use_bias = confi.get('Dense').get('use_bias'),\n",
    "                                        kernel_initializer=kernel_initializer,\n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        kernel_regularizer = confi.get('Dense').get('kernel_regularizer'),\n",
    "                                        bias_regularizer = confi.get('Dense').get('bias_regularizer'),\n",
    "                                        activity_regularizer = confi.get('Dense').get('activity_regularizer'),\n",
    "                                        kernel_constraint = confi.get('Dense').get('kernel_constraint'),\n",
    "                                        bias_constraint = confi.get('Dense').get('bias_constraint')\n",
    "                                        ))\n",
    "                                        \n",
    "        # model.add(keras.layers.Dropout(0.001))\n",
    "        # print()\n",
    "\n",
    "# Out\n",
    "model.add(keras.layers.Dense(   units=1,\n",
    "                                activation='linear',\n",
    "                                kernel_initializer=kernel_initializer,\n",
    "                                bias_initializer=bias_initializer\n",
    "                                ))\n",
    "                                \n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[mae,rmse]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(\n",
    "                                            monitor=\"loss\",\n",
    "                                            min_delta=0,\n",
    "                                            patience=patience,\n",
    "                                            verbose=0,\n",
    "                                            mode=\"min\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=False,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epocas:20\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1,\n",
    "                    verbose=0,\n",
    "                    workers=2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "print(f'Total epocas:{len(history.epoch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(x_train, verbose=0).reshape(-1)\n",
    "testPredict = model.predict(x_vasl, verbose=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data de test\n",
    "trainind_pd = pd.DataFrame(trainPredict,\n",
    "                            index = pd_model_id[-(trainPredict.shape[0]+prediction_order):-(prediction_order)].index,\n",
    "                            columns=['prediction']\n",
    "                            )\n",
    "\n",
    "trainind_pd[y_output] = y_train.reshape(-1)\n",
    "trainind_pd['type'] = 'training'\n",
    "trainind_pd['precipitation_ann_t'] = np.nan\n",
    "\n",
    "trainind_pd['id_point'] = id_point\n",
    "\n",
    "trainind_pd[['prediction_ann','ndvi_prediction']] = ndvi_transformacion.inverse_transform(trainind_pd[['precipitation_ann_t','prediction']])\n",
    "trainind_pd[['prediction_ann','ndvi_media']] = ndvi_transformacion.inverse_transform(trainind_pd[['precipitation_ann_t',y_output]])\n",
    "\n",
    "trainind_pd = trainind_pd.reset_index(drop=False)[['id_point', 'periodo','type','ndvi_prediction','ndvi_media']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion entrenamiento\n",
    "trainig_metrics = metrics(observado=trainind_pd.ndvi_media,\n",
    "                          prediccion=trainind_pd.ndvi_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data de test\n",
    "validation_pd = pd.DataFrame(testPredict,\n",
    "                            index = pd_model_id[-prediction_order:].index,\n",
    "                            columns=['prediction']\n",
    "                            )\n",
    "\n",
    "validation_pd[y_output] = y_vasl.reshape(-1)\n",
    "validation_pd['type'] = 'validation'\n",
    "validation_pd['precipitation_ann_t'] = np.nan\n",
    "\n",
    "validation_pd['id_point'] = id_point\n",
    "\n",
    "validation_pd[['prediction_ann','ndvi_prediction']] = ndvi_transformacion.inverse_transform(validation_pd[['precipitation_ann_t','prediction']])\n",
    "validation_pd[['prediction_ann','ndvi_media']] = ndvi_transformacion.inverse_transform(validation_pd[['precipitation_ann_t',y_output]])\n",
    "\n",
    "validation_pd = validation_pd.reset_index(drop=False)[['id_point', 'periodo','type','ndvi_prediction','ndvi_media']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion entrenamiento\n",
    "validation_metrics = metrics(observado=validation_pd.ndvi_media,\n",
    "                          prediccion=validation_pd.ndvi_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sección Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exogena = pd_model_id[-prediction_order:][[exogena]]\n",
    "data_exogena[y_output] = np.nan\n",
    "data_predict = pd_model_id[pd_model_id.index < data_exogena.index.min()][[y_output,exogena]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order,exogena,y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order,exogena,y_output)\n",
    "\n",
    "pd_test = pd_test.rename(columns={y_output:'prediction'})\n",
    "pd_test['type'] = 'test'\n",
    "\n",
    "pd_test[y_output] = pd_model_id[pd_model_id.index > trainind_pd.periodo.max()][y_output]\n",
    "\n",
    "pd_test['id_point'] = id_point\n",
    "\n",
    "pd_test[['prediction_ann','ndvi_prediction']] = ndvi_transformacion.inverse_transform(pd_test[['precipitation_ann_t','prediction']])\n",
    "pd_test[['prediction_ann','ndvi_media']] = ndvi_transformacion.inverse_transform(pd_test[['precipitation_ann_t',y_output]])\n",
    "\n",
    "pd_test = pd_test.reset_index(drop=False)[['id_point', 'periodo','type','ndvi_prediction','ndvi_media']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion entrenamiento\n",
    "test_metrics = metrics(observado=pd_test.ndvi_media,\n",
    "                        prediccion=pd_test.ndvi_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del modelo\n",
    "dict_metrics = {'epocas':[len(history.epoch)],\n",
    "                'prediction_order':[prediction_order],\n",
    "                'auto_order':[auto_order],\n",
    "                'exog_order':[exog_order],\n",
    "                'exog_delay':[exog_delay],\n",
    "                'activation':[activation[0]],\n",
    "                'id_point':[id_point],\n",
    "                'n_neurons':str(n_neurons),\n",
    "                'capas':[len(n_neurons)],\n",
    "                'training_mse':[trainig_metrics[\"mse\"]],\n",
    "                'training_rmse':[trainig_metrics[\"rmse\"]],\n",
    "                'training_mae':[trainig_metrics[\"mae\"]],\n",
    "                'trainig_mape':[trainig_metrics['mape']],\n",
    "                'trainig_r':[trainig_metrics['r2']],\n",
    "                'validation_mse':[validation_metrics[\"mse\"]],\n",
    "                'validation_rmse':[validation_metrics[\"rmse\"]],\n",
    "                'validation_mae':[validation_metrics[\"mae\"]],\n",
    "                'validation_mape':[validation_metrics['mape']],\n",
    "                'validation_r':[validation_metrics['r2']],\n",
    "                'test_mse':[test_metrics[\"mse\"]],\n",
    "                'test_rmse':[test_metrics[\"rmse\"]],\n",
    "                'test_mae':[test_metrics[\"mae\"]],\n",
    "                'test_mape':[test_metrics['mape']],\n",
    "                'test_r':[test_metrics['r2']]\n",
    "                }\n",
    "\n",
    "experimento_pd = pd.DataFrame.from_dict(dict_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronóstico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predict = pd_model_id[[y_output,exogena]]\n",
    "\n",
    "data_exogena = pd_precipitacion[(pd_precipitacion.periodo > data_predict.index.max()) & (pd_precipitacion.id_point==id_point)][[exogena,'periodo']]\n",
    "data_exogena.index = pd.to_datetime(data_exogena.periodo)\n",
    "data_exogena[y_output] = np.nan\n",
    "data_exogena = data_exogena.sort_index()[[exogena,y_output]]\n",
    "\n",
    "pd_prediction = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order, exogena, y_output)\n",
    "pd_prediction = pd_prediction.rename(columns={y_output:'prediction'})\n",
    "pd_prediction['type'] = 'prediction'\n",
    "pd_prediction['id_point'] = id_point\n",
    "\n",
    "\n",
    "pd_prediction[['prediction_ann','ndvi_prediction']] = ndvi_transformacion.inverse_transform(pd_prediction[['precipitation_ann_t','prediction']])\n",
    "pd_prediction['ndvi_media'] = np.nan\n",
    "\n",
    "pd_prediction = pd_prediction.reset_index(drop=False)[['id_point', 'periodo','type','ndvi_prediction','ndvi_media']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniendo informacion\n",
    "pd_summary = pd.concat([trainind_pd[list(pd_prediction)], \n",
    "                        pd_test[list(pd_prediction)], \n",
    "                        validation_pd[list(pd_prediction)], \n",
    "                        pd_prediction[list(pd_prediction)]\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logica de guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: -0.3385701394029088; Best Model: 0.4687721378276808\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if os.listdir(f'{DIR}{experimento}') == []:\n",
    "\n",
    "    # Modelo\n",
    "    model.save(f'{DIR}{experimento}/model.h5')\n",
    "\n",
    "    # Pesos\n",
    "    model.save_weights(f'{DIR}{experimento}/weights.h5')\n",
    "\n",
    "    # History\n",
    "    with open(f'{DIR}{experimento}/history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # guardando resultados\n",
    "    pd_summary.to_pickle(f'{DIR}{experimento}/predicciones.pkl')\n",
    "\n",
    "else:\n",
    "    files = [x for x in os.listdir(f'{DIR}{experimento}') if x.find('summary')!=-1 ]\n",
    "    total_summary = pd.concat([pd.read_csv(f'{DIR}{experimento}/{file}') for file in files])\n",
    "    print( f\"Actual: {validation_metrics['r2']}; Best Model: {total_summary.validation_r.max()}\" )\n",
    "\n",
    "    if validation_metrics['r2'] > total_summary.validation_r.max(): \n",
    "\n",
    "        # Modelo\n",
    "        model.save(f'{DIR}{experimento}/model.h5')\n",
    "\n",
    "        # Pesos\n",
    "        model.save_weights(f'{DIR}{experimento}/weights.h5')\n",
    "\n",
    "        # History\n",
    "        with open(f'{DIR}{experimento}/history.pkl', 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "        \n",
    "        # guardando resultados\n",
    "        pd_summary.to_pickle(f'{DIR}{experimento}/predicciones.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "experi = f'{DIR}{experimento}/{id_point}_{len(n_neurons)}_{activation[0]}_{prediction_order}_{auto_order}_{exog_order}_{exog_delay}'\n",
    "experimento_pd.to_csv(f'{experi}_summary.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ssev_analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7962acab993cb47be2d59fc45292cc4ac077641aa9e9bf66ffc44da07d377859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
