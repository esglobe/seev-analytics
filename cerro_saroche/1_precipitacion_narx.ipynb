{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis Precipitación Total Parque Nacional Cerro Saroche\n",
    "\n",
    "**PROYECTO:** SISTEMA PARA EL SEGUIMIENTO DE ECOSISTEMAS VENEZOLANOS \\\n",
    "**AUTOR:** Javier Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Directorio actual:  /media/javier/Compartida/doctorado/ssev-analytics/cerro_saroche\n",
      "> Directorio actual:  /media/javier/Compartida/doctorado/ssev-analytics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print('> Directorio actual: ', os.getcwd())  \n",
    "os.chdir('../')\n",
    "print('> Directorio actual: ', os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MONGO import CONEXION\n",
    "from utils.UTILS import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando Coenxión con Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meteorological', 'estimateSSTNino34', 'SSTNino34']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando la conexión con MongoDB\n",
    "db = CONEXION.conexion()\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargando la Información Precipitación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parque\n",
    "park = 'cerro_saroche'\n",
    "id_point = 1\n",
    "\n",
    "y_output = 'precip_t'\n",
    "exogena = 'oni'\n",
    "\n",
    "prediction_order = 12 # rango de prediccion\n",
    "auto_order = 20*12 # componente autoregresiva\n",
    "exog_order =  10*12# componente exogena qm\n",
    "exog_delay = 1# componente exogena dm\n",
    "\n",
    "f_activation = 'sigmoid'\n",
    "\n",
    "# Parametros de modelos\n",
    "patience = 15\n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorio del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = f'./{park}/'\n",
    "experimento = f'experiments/narx/precipitacion/id_point_{id_point}'\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'{DIR}{experimento}')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando consulta\n",
    "meteorological = db.meteorological.find({\"park\":park, 'id_point':id_point})\n",
    "\n",
    "# Generando pandas dataframe\n",
    "data_pandas = pd.DataFrame([file for file in meteorological])\n",
    "data_pandas['periodo'] = data_pandas.time.apply(lambda x: datetime.fromordinal(x))\n",
    "data_pandas['mes_year'] =  data_pandas['periodo'].dt.strftime('%B-%Y')\n",
    "data_pandas.index = pd.to_datetime(data_pandas.periodo)\n",
    "#data_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio Precipitación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_precipitacion = data_pandas[['id_point', 'latitud', 'longitud',\n",
    "                                'precipitacion_mm']]\n",
    "#pd_precipitacion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando data SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando consulta\n",
    "data_sst = db.estimateSSTNino34.find()\n",
    "\n",
    "# Generando pandas dataframe\n",
    "pd_sst = pd.DataFrame([file for file in data_sst])[['nino34_mean','oni','time']]\n",
    "pd_sst['periodo'] = pd_sst.time.apply(lambda x: datetime.fromordinal(x))\n",
    "pd_sst.index = pd.to_datetime(pd_sst.periodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion\n",
    "oni_transformacion = MinMaxScaler() #LogMinimax.create( pd_sst.oni.to_numpy() )\n",
    "oni_transformacion.fit(pd_sst[['oni']])\n",
    "\n",
    "pd_sst['oni'] = oni_transformacion.transform( pd_sst[['oni']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion\n",
    "sst_transformacion = LogMinimax.create( pd_sst.nino34_mean.to_numpy() )\n",
    "\n",
    "pd_sst['sst_t'] = sst_transformacion.transformacion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "pd_model = pd.merge(pd_precipitacion.reset_index(drop=False),pd_sst[['oni','sst_t']].reset_index(drop=False),\n",
    "                    on=['periodo'],\n",
    "                    how='left'\n",
    "                    )\n",
    "\n",
    "# Pronostico\n",
    "pd_sst_pron = pd_sst[['periodo','oni','sst_t']][pd_sst.periodo > pd_model.periodo.max()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>id_point</th>\n",
       "      <th>latitud</th>\n",
       "      <th>longitud</th>\n",
       "      <th>precipitacion_mm</th>\n",
       "      <th>oni</th>\n",
       "      <th>sst_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.913065</td>\n",
       "      <td>0.543835</td>\n",
       "      <td>0.526009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.081278</td>\n",
       "      <td>0.506771</td>\n",
       "      <td>0.549133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.413783</td>\n",
       "      <td>0.497505</td>\n",
       "      <td>0.585536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.895653</td>\n",
       "      <td>0.476123</td>\n",
       "      <td>0.698842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-05-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>2.909450</td>\n",
       "      <td>0.444048</td>\n",
       "      <td>0.676378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.046146</td>\n",
       "      <td>0.228083</td>\n",
       "      <td>0.280822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.275602</td>\n",
       "      <td>0.234498</td>\n",
       "      <td>0.335206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.418985</td>\n",
       "      <td>0.222381</td>\n",
       "      <td>0.424597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>3.615426</td>\n",
       "      <td>0.205987</td>\n",
       "      <td>0.500842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.31</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>0.708163</td>\n",
       "      <td>0.220242</td>\n",
       "      <td>0.522145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>629 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       periodo  id_point  latitud  longitud  precipitacion_mm       oni  \\\n",
       "0   1970-01-01         1    10.31    -69.83          0.913065  0.543835   \n",
       "1   1970-02-01         1    10.31    -69.83          0.081278  0.506771   \n",
       "2   1970-03-01         1    10.31    -69.83          0.413783  0.497505   \n",
       "3   1970-04-01         1    10.31    -69.83          0.895653  0.476123   \n",
       "4   1970-05-01         1    10.31    -69.83          2.909450  0.444048   \n",
       "..         ...       ...      ...       ...               ...       ...   \n",
       "624 2022-01-01         1    10.31    -69.83          0.046146  0.228083   \n",
       "625 2022-02-01         1    10.31    -69.83          0.275602  0.234498   \n",
       "626 2022-03-01         1    10.31    -69.83          0.418985  0.222381   \n",
       "627 2022-04-01         1    10.31    -69.83          3.615426  0.205987   \n",
       "628 2022-05-01         1    10.31    -69.83          0.708163  0.220242   \n",
       "\n",
       "        sst_t  \n",
       "0    0.526009  \n",
       "1    0.549133  \n",
       "2    0.585536  \n",
       "3    0.698842  \n",
       "4    0.676378  \n",
       "..        ...  \n",
       "624  0.280822  \n",
       "625  0.335206  \n",
       "626  0.424597  \n",
       "627  0.500842  \n",
       "628  0.522145  \n",
       "\n",
       "[629 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustando modelo NARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion\n",
    "transformacion = LogMinimax.create( pd_precipitacion.precipitacion_mm.to_numpy() )\n",
    "\n",
    "pd_model['precip_t'] = transformacion.transformacion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo según ID point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_model_id = pd_model[pd_model.id_point==id_point]\n",
    "pd_model_id.index = pd.to_datetime(pd_model_id.periodo)\n",
    "pd_model_id = pd_model_id[[y_output,exogena]]\n",
    "\n",
    "#pd_model_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiendo estructura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pd_model_id,exog_order,auto_order,exog_delay,prediction_order):\n",
    "    \"\"\"\n",
    "    Funcion para dale estructura a los datos\n",
    "    \"\"\"\n",
    "\n",
    "    min_index = max([exog_order,auto_order])\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for t in pd_model_id[min_index:].index:\n",
    "\n",
    "        #t = pd_model_id[min_index:].index.min()\n",
    "\n",
    "        to_split = pd_model_id[[y_output,exogena]]\n",
    "        to_split = to_split[(t-pd.DateOffset(months=auto_order)):t].copy()\n",
    "\n",
    "        # Exogena\n",
    "        x_exo = to_split[exogena][(t-pd.DateOffset(months=exog_delay+exog_order)):(t-pd.DateOffset(months=exog_delay+1))]\\\n",
    "                            .to_numpy()\\\n",
    "                            .astype(float)\\\n",
    "                            .reshape(-1)\n",
    "\n",
    "        # Auto\n",
    "        x_auto = to_split[y_output][:-1]\\\n",
    "                    .to_numpy()\\\n",
    "                    .astype(float)\\\n",
    "                    .reshape(-1)\n",
    "\n",
    "        x_data.append(np.concatenate([x_exo, x_auto],axis=None))\n",
    "        y_data.append(to_split[y_output][-1])\n",
    "\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = split_data(pd_model_id,exog_order,auto_order,exog_delay,prediction_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_data[:-prediction_order]\n",
    "x_vasl = x_data[-prediction_order:]\n",
    "\n",
    "y_train = y_data[:-prediction_order]\n",
    "y_vasl = y_data[-prediction_order:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo NARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrícas\n",
    "mae = keras.metrics.MeanAbsoluteError()\n",
    "rmse = keras.metrics.RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "confi = {'Input':{'batch_size':None,\n",
    "                'name':'input',\n",
    "                'dtype':None,\n",
    "                'sparse':None,\n",
    "                'tensor':None,\n",
    "                'ragged':None,\n",
    "                'type_spec':None},\n",
    "        'Dense':{'use_bias':True,\n",
    "                'kernel_regularizer':None,\n",
    "                'bias_regularizer':None,\n",
    "                'activity_regularizer':None,\n",
    "                'kernel_constraint':None,\n",
    "                'bias_constraint':None\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = int(2*x_train.shape[-1]/3)\n",
    "n_neurons = [int(3*total/6), int(2*total/6), int(1*total/6), 1]\n",
    "\n",
    "activation = len(n_neurons)*[f_activation]\n",
    "kernel_initializer = 'lecun_normal'\n",
    "bias_initializer = 'zeros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "80\n",
      "40\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Entradas\n",
    "model.add(keras.layers.Input(shape=(x_train.shape[-1],),\n",
    "                                    batch_size = confi.get('Input').get('batch_size'),\n",
    "                                    name = confi.get('Input').get('name'),\n",
    "                                    dtype = confi.get('Input').get('dtype'),\n",
    "                                    sparse = confi.get('Input').get('sparse'),\n",
    "                                    tensor = confi.get('Input').get('tensor'),\n",
    "                                    ragged = confi.get('Input').get('ragged'),\n",
    "                                    type_spec = confi.get('Input').get('type_spec')\n",
    "                                    ))\n",
    "\n",
    "# Hidden Leyers\n",
    "if len(n_neurons)>=1:\n",
    "    for index in list( range(0, len(n_neurons)) ):\n",
    "\n",
    "        model.add(keras.layers.Dense(   units=n_neurons[index],\n",
    "                                        activation=activation[index],\n",
    "                                        use_bias = confi.get('Dense').get('use_bias'),\n",
    "                                        kernel_initializer=kernel_initializer,\n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        kernel_regularizer = confi.get('Dense').get('kernel_regularizer'),\n",
    "                                        bias_regularizer = confi.get('Dense').get('bias_regularizer'),\n",
    "                                        activity_regularizer = confi.get('Dense').get('activity_regularizer'),\n",
    "                                        kernel_constraint = confi.get('Dense').get('kernel_constraint'),\n",
    "                                        bias_constraint = confi.get('Dense').get('bias_constraint')\n",
    "                                        ))\n",
    "                                        \n",
    "        model.add(keras.layers.Dropout(0.001))\n",
    "\n",
    "# Out\n",
    "model.add(keras.layers.Dense(   units=1,\n",
    "                                activation='linear',\n",
    "                                kernel_initializer=kernel_initializer,\n",
    "                                bias_initializer=bias_initializer\n",
    "                                ))\n",
    "                                \n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[mae,rmse]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(\n",
    "                                            monitor=\"loss\",\n",
    "                                            min_delta=0,\n",
    "                                            patience=patience,\n",
    "                                            verbose=0,\n",
    "                                            mode=\"min\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=False,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1,\n",
    "                    verbose=0,\n",
    "                    workers=2,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epocas:194\n"
     ]
    }
   ],
   "source": [
    "print(f'Total epocas:{len(history.epoch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(x_train, verbose=0).reshape(-1)\n",
    "testPredict = model.predict(x_vasl, verbose=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data de test\n",
    "trainind_pd = pd.DataFrame(trainPredict,\n",
    "                            index = pd_model_id[-(trainPredict.shape[0]+prediction_order):-(prediction_order)].index,\n",
    "                            columns=['prediction']\n",
    "                            )\n",
    "\n",
    "trainind_pd[y_output] = y_train.reshape(-1)\n",
    "trainind_pd['type'] = 'training'\n",
    "\n",
    "trainind_pd['precipitacion_mm'] = trainind_pd[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "trainind_pd['prediction_precipitacion_mm'] = trainind_pd['prediction'].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "trainind_pd = pd.merge(trainind_pd,pd_model_id[[exogena]].reset_index(drop=False),\n",
    "                        on=['periodo'],\n",
    "                        how='left')\n",
    "\n",
    "trainind_pd.index = pd.to_datetime(trainind_pd.periodo)\n",
    "\n",
    "# Validacion entrenamiento\n",
    "trainig_metrics = metrics(observado=trainind_pd.precipitacion_mm,\n",
    "                          prediccion=trainind_pd.prediction_precipitacion_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data de Validacion\n",
    "validation_pd = pd.DataFrame(testPredict,\n",
    "                            index = pd_model_id[-prediction_order:].index,\n",
    "                            columns=['prediction']\n",
    "                            )\n",
    "\n",
    "validation_pd[y_output] = y_vasl.reshape(-1)\n",
    "validation_pd['type'] = 'validation'\n",
    "\n",
    "validation_pd['precipitacion_mm'] = validation_pd[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "validation_pd['prediction_precipitacion_mm'] = validation_pd['prediction'].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "validation_pd = pd.merge(validation_pd,pd_model_id[[exogena]].reset_index(drop=False),\n",
    "                        on=['periodo'],\n",
    "                        how='left')\n",
    "\n",
    "# Validacion entrenamiento\n",
    "validation_metrics = metrics(observado=validation_pd.precipitacion_mm,\n",
    "                          prediccion=validation_pd.prediction_precipitacion_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order):\n",
    "  \"\"\"\n",
    "  Funcion para predecir a un paso\n",
    "  \"\"\"\n",
    "  data_predict = data_predict.copy()\n",
    "  data_predict['type'] = 'data_in'\n",
    "\n",
    "  for t in data_exogena.index:\n",
    "    \n",
    "      x_data_test, y_data_test = split_data(data_predict,\n",
    "                                          exog_order,\n",
    "                                          auto_order,\n",
    "                                          exog_delay,\n",
    "                                          prediction_order)\n",
    "\n",
    "\n",
    "      predit = model.predict(x_data_test[-1].reshape(1, x_data_test.shape[1]), verbose=0).reshape(-1)\n",
    "      exo = data_exogena[data_exogena.index==t][exogena][0]\n",
    "\n",
    "      data_test = pd.DataFrame({'periodo':t, y_output:predit, exogena:exo,'type':'data_out'},index=[0])\n",
    "      data_test.index = pd.to_datetime(data_test.periodo)\n",
    "\n",
    "      data_predict = pd.concat([data_predict,\n",
    "                                data_test[list(data_predict)]\n",
    "                              ])\n",
    "\n",
    "  return data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exogena = pd_model_id[-prediction_order:][[exogena]]\n",
    "data_predict = pd_model_id[pd_model_id.index < data_exogena.index.min()][[y_output,exogena]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order)\n",
    "\n",
    "pd_test = pd_test[pd_test.type=='data_out'].rename(columns={y_output:'prediction'})\n",
    "pd_test['type'] = 'test'\n",
    "\n",
    "pd_test['precip_t'] = pd_model_id[pd_model_id.index > trainind_pd.periodo.max()][y_output]\n",
    "\n",
    "pd_test['precipitacion_mm'] = pd_test[y_output].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "pd_test['prediction_precipitacion_mm'] = pd_test['prediction'].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "# Validacion entrenamiento\n",
    "test_metrics = metrics(observado=pd_test.precipitacion_mm,\n",
    "                        prediccion=pd_test.prediction_precipitacion_mm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del modelo\n",
    "dict_metrics = {'epocas':[len(history.epoch)],\n",
    "                'prediction_order':[prediction_order],\n",
    "                'auto_order':[auto_order],\n",
    "                'exog_order':[exog_order],\n",
    "                'exog_delay':[exog_delay],\n",
    "                'activation':[activation[0]],\n",
    "                'id_point':[id_point],\n",
    "                'n_neurons':str(n_neurons),\n",
    "                'capas':[len(n_neurons)],\n",
    "                'training_mse':[history.history[\"loss\"][-1]],\n",
    "                'training_rmse':[history.history[\"root_mean_squared_error\"][-1]],\n",
    "                'training_mae':[history.history[\"mean_absolute_error\"][-1]],\n",
    "                'trainig_mape':[trainig_metrics['mape']],\n",
    "                'trainig_r':[trainig_metrics['r2']],\n",
    "                'validation_mse':[validation_metrics[\"mse\"]],\n",
    "                'validation_rmse':[validation_metrics[\"rmse\"]],\n",
    "                'validation_mae':[validation_metrics[\"mae\"]],\n",
    "                'validation_mape':[validation_metrics['mape']],\n",
    "                'validation_r':[validation_metrics['r2']],\n",
    "                'test_mse':[test_metrics[\"mse\"]],\n",
    "                'test_rmse':[test_metrics[\"rmse\"]],\n",
    "                'test_mae':[test_metrics[\"mae\"]],\n",
    "                'test_mape':[test_metrics['mape']],\n",
    "                'test_r':[test_metrics['r2']]\n",
    "                }\n",
    "\n",
    "experimento_pd = pd.DataFrame.from_dict(dict_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronóstico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predict = pd_model_id[[y_output,exogena]]\n",
    "data_exogena = pd_sst_pron[pd_sst_pron.index>data_predict.index.max()][[exogena]]\n",
    "\n",
    "pd_prediction = predict_one_stap_narx(model,data_predict,data_exogena,exog_order,auto_order,exog_delay,prediction_order)\n",
    "\n",
    "pd_prediction = pd_prediction[pd_prediction.type=='data_out'].rename(columns={y_output:'prediction'})\n",
    "pd_prediction['type'] = 'prediction'\n",
    "\n",
    "pd_prediction['precipitacion_mm'] = np.nan\n",
    "pd_prediction['prediction_precipitacion_mm'] = pd_prediction['prediction'].apply(lambda x: transformacion.inversa(x) if np.isnan(x)==False else np.nan )\n",
    "\n",
    "pd_prediction['precip_t'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniendo informacion\n",
    "pd_summary = pd.concat([trainind_pd[list(pd_prediction)], \n",
    "                        pd_test[list(pd_prediction)], \n",
    "                        validation_pd[list(pd_prediction)], \n",
    "                        pd_prediction[list(pd_prediction)]\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logica de guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: -0.25475301129712524; Best Model: -0.0646371988983034\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if os.listdir(f'{DIR}{experimento}') == []:\n",
    "\n",
    "    # Modelo\n",
    "    model.save(f'{DIR}{experimento}/model.h5')\n",
    "\n",
    "    # Pesos\n",
    "    model.save_weights(f'{DIR}{experimento}/weights.h5')\n",
    "\n",
    "    # History\n",
    "    with open(f'{DIR}{experimento}/history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # guardando resultados\n",
    "    pd_summary.to_pickle(f'{DIR}{experimento}/predicciones.pkl')\n",
    "\n",
    "else:\n",
    "    files = [x for x in os.listdir(f'{DIR}{experimento}') if x.find('summary')!=-1 ]\n",
    "    total_summary = pd.concat([pd.read_csv(f'{DIR}{experimento}/{file}') for file in files])\n",
    "    print( f\"Actual: {validation_metrics['r2']}; Best Model: {total_summary.validation_r.max()}\" )\n",
    "\n",
    "    if validation_metrics['r2'] > total_summary.validation_r.max(): \n",
    "\n",
    "        # Modelo\n",
    "        model.save(f'{DIR}{experimento}/model.h5')\n",
    "\n",
    "        # Pesos\n",
    "        model.save_weights(f'{DIR}{experimento}/weights.h5')\n",
    "\n",
    "        # History\n",
    "        with open(f'{DIR}{experimento}/history.pkl', 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "        \n",
    "        # guardando resultados\n",
    "        pd_summary.to_pickle(f'{DIR}{experimento}/predicciones.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardando Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "experi = f'{DIR}{experimento}/{id_point}_{len(n_neurons)}_{activation[0]}_{prediction_order}_{auto_order}_{exog_order}_{exog_delay}'\n",
    "experimento_pd.to_csv(f'{experi}_summary.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ssev_analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7962acab993cb47be2d59fc45292cc4ac077641aa9e9bf66ffc44da07d377859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
